---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# FunQ <img src="man/figures/logo.png" align="right" height="150" alt="funq website" /></a>

<!-- badges: start -->
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Package Version](https://img.shields.io/badge/version-0.1.10-blue.svg)](https://cran.r-project.org/package=yourPackageName)
[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg)](https://www.tidyverse.org/lifecycle/)
<!-- badges: end -->

`FunQ` is an R package that provides methodologies for functional data analysis from a quantile regression perspective. It currently implements the Functional Quantile Principal Component Analysis (FQPCA) methodology explained in detail in [this paper](https://doi.org/10.1093/biostatistics/kxae040), which extends traditional Functional Principal Component Analysis (FPCA) to the quantile regression framework. This allows for a richer understanding of functional data by capturing the full curve and time-specific probability distributions underlying individual measurements, especially useful when data distributions are skewed, vary across subjects, or contain outliers.

## Installation

You can install the development version of FunQ from [GitHub](https://github.com/) with:

```{r, echo=T, eval=F}
# install.packages("devtools")
devtools::install_github("alvaromc317/FunQ")
```

## Overview

`FunQ` provides functions for implementing the Functional Quantile Principal Component Analysis methodology. Key features include:

* Flexible Input Formats: accepts functional data as an $(I\times T)$ matrix, a `tf` functional vector, or a `dataframe` containing a column with a `tf` functional vector (using the `data` and `colname` parameters).
* Handling Missing Data: capable of dealing with irregular time grids and missing data.
* Smoothness Control: allows control over the smoothness of the results using two approaches:
  * Spline Degrees of Freedom: Using the `splines.df` parameter to set the degrees of freedom in spline basis reconstruction.
  * Second Derivative Penalty: experimental method involving a second derivative penalty on the spline coefficients by setting `penalized = TRUE` and specifying `lambda.ridge`. Requires setting `splines.method = "conquer"`.
  
Additionally, the package includes functions for cross-validation of parameters such as `splines.df` (`cross_validation_df`) and `lambda.ridge` (`cross_validation_lambda`), using the quantile error as the reference prediction error metric (`quantile_error` function).

## Example 1: basics of `fqpca`

Let's start by loading the necessary libraries:

```{r, echo=T, eval=F}
# We use the tidy structure of the tidyfun package to deal with the functional data
devtools::install_github("tidyfun/tidyfun")
```

```{r, message=FALSE, warning=F}
library(FunQ)
library(fda)
library(tidyverse)
library(tidyfun)
```

We use the `tidyfun` package for handling functional data structures.

### Generating synthetic data

We generate a synthetic dataset with 200 observations taken every 10 minutes over a day (144 time points). The data follows the formula:

$$x_i(t) = c_{i1}\text{sin}(t) + c_{i2}\text{cos}(t) + \frac{t}{50} + \varepsilon_i$$

where

* $c_1\sim N(0, 1)$ 
* $c_2\sim N(0, 1)$ 
* $\varepsilon_i\sim\chi^2(3)$


```{r, fig.height = 4, fig.width = 10}
set.seed(5)

n.obs = 200
n.time = 144

# Generate scores
c1.vals = rnorm(n.obs)
c2.vals = rnorm(n.obs)

# Generate pc's
pc1 = sin(seq(0, 2*pi, length.out = n.time))
pc2 = cos(seq(0, 2*pi, length.out = n.time))

# Generate data
Y <- c1.vals * matrix(pc1, nrow = n.obs, ncol=n.time, byrow = TRUE) +
  c2.vals * matrix(pc2, nrow = n.obs, ncol=n.time, byrow = TRUE) +
  matrix(seq(1/n.time, 1, length.out=n.time), nrow = n.obs, ncol=n.time, byrow = TRUE)

# Add noise
Y <- Y + matrix(rchisq(n.obs * n.time, 3), nrow = n.obs)

Y[1:50,] %>% tf::tfd() %>% plot(alpha=0.2)
```

The above plot visualizes a subset of the data generated this way. To simulate missing data and test the method's robustness, we introduce 50% missing observations:

```{r}
# Add missing observations
Y[sample(n.obs*n.time, as.integer(0.5*n.obs*n.time))] <- NA
```

### Aplying `fqpca`

We split the data into training and testing sets:

```{r}
Y.train = Y[1:150,]
Y.test = Y[151:n.obs,]
```

We then apply the `fqpca` function to decompose the data in terms of the median (`quantile.value = 0.5`), providing a robust alternative to mean-based predictions:

```{r}
results = fqpca(data=Y.train, npc=2, quantile.value=0.5, periodic=FALSE)

intercept = results$intercept
loadings = results$loadings
scores = results$scores

# Recover Y.train based on decomposition
Y.train.estimated = fitted(results, pve = 0.95)
```

For new observations, we can compute scores using the existing loadings:

```{r}
scores.test = predict(results, newdata=Y.test)
Y.test.estimated = scores.test %*% t(loadings)
```

Plotting the computed loadings:

```{r, fig.height = 4, fig.width = 10}
plot(results, pve=0.95)
```

Calculating the quantile error between the reconstructed and true data:

```{r}
quantile_error(Y=Y.train, Y.pred=Y.train.estimated, quantile.value=0.5)
```

### Cross validation 

We perform cross-validation to optimize the `splines.df` parameter:

```{r}
splines.df.grid = c(5, 10, 15, 20)
cv_result = fqpca_cv_df(Y, splines.df.grid=splines.df.grid, n.folds=3, verbose.cv=F, periodic=F)
cv_result$error.matrix
```

The dimensions of the error matrix are `(length(splines.df.grid), n.folds)`. We can determine the optimal degrees of freedom by taking the mean of each row and picking the minimum.

```{r}
optimal_df = which.min(rowMeans(cv_result$error.matrix))
paste0('Optimal number of degrees of freedom: ', splines.df.grid[optimal_df])
```

## Example 2: Using penalized `fqpca`

In this example, we demonstrate how to apply the second derivative penalty approach by setting penalized = TRUE and lambda.ridge = 0.001. This method is experimental but has shown promising results.

```{r}
# Apply fqpca with penalization
results_penalized <- fqpca(data = Y.train, npc = 2, quantile.value = 0.5, penalized = TRUE, lambda.ridge = 0.001, splines.method = "conquer", periodic=F)

# Reconstruct the training data
Y.train.estimated_penalized <- fitted(results_penalized, pve = 0.95)

```

The package also includes a function that allows to perform cross validation on the hyper-parameter controlling the effect of a second derivative penalty on the splines.

```{r}
cv_result = fqpca_cv_lambda(Y, lambda.grid=c(0, 1e-5, 1e-3), n.folds=3, verbose.cv=F, periodic=F)
cv_result$error.matrix
```

The dimensions of the error matrix are `(length(lambda.grid), n.folds)`.

## Example 3: The Canadian Weather dataset

Let's explore the use of `FunQ` with the well-known Canadian Weather dataset. `FunQ` can handle:

* Data matrices
* `tf` vectors from the `tidyfun` package
* `data.frames` containing `tf` vectors
 
### Loading and Visualizing Data

```{r}
matrix.data = t(fda::CanadianWeather$dailyAv[,,1])
data = tibble(temperature = tf::tfd(matrix.data, arg = 1:365),
              province = CanadianWeather$province)
head(data)
```

Plotting the temperature data:

```{r, fig.height = 4, fig.width = 10}
data %>% 
  ggplot(aes(y=temperature, color=province)) + 
  geom_spaghetti() + 
  theme_light()
```

### Cross-Validation for `splines.df`

We perform cross-validation to find the optimal `splines.df` value:

```{r}
splines.df.grid = c(5, 10, 15, 20)
cv_result = fqpca_cv_df(data=data, colname='temperature', splines.df.grid=splines.df.grid, n.folds=3, verbose.cv=F)
optimal_df = splines.df.grid[which.min(rowMeans(cv_result$error.matrix))]
paste0('Optimal number of degrees of freedom: ', optimal_df)
```

### Building the Final Model

Using the optimal `splines.df`, we fit the fqpca model and examine the explained variance:

```{r}
results = fqpca(data=data$temperature, npc=10, quantile.value=0.5, splines.df=optimal_df, seed=5)
cumsum(results$pve)
```

With 3 components, we achieve over 99% of explained variability. Plotting the components:

```{r, fig.height = 4, fig.width = 10}
plot(results, pve = 0.99)
```

### Computing various quantile levels

One of the advantages of `FunQ` is its ability to recover the distribution of data at different quantile levels. We can compute the 10%, 50%, and 90% quantile curves:

```{r}
m01 = fqpca(data=data$temperature, npc=10, quantile.value=0.1, splines.df=15, seed=5)
m05 = fqpca(data=data$temperature, npc=10, quantile.value=0.5, splines.df=15, seed=5)
m09 = fqpca(data=data$temperature, npc=10, quantile.value=0.9, splines.df=15, seed=5)

Y01 = fitted(m01, pve = 0.99)
Y05 = fitted(m05, pve = 0.99)
Y09 = fitted(m09, pve = 0.99)
```

Now given an observation we can visualize it's quantile curves along with the raw data

```{r, echo=F, warning=F, fig.height = 4, fig.width = 10}
idx_curves = c(5, 10, 15)
idx_curves_named = rownames(matrix.data)[idx_curves]

tmpy = matrix.data %>% 
  unname() %>% 
  as_tibble() %>% 
  slice(idx_curves) %>% 
  mutate(Quantile='Original data',
         Source='Original data',
         link_curve = idx_curves_named)

tmpy_01 = Y01 %>% 
  as_tibble() %>% 
  slice(idx_curves) %>% 
  mutate(Quantile='10%',
         Source='FQPCA',
         link_curve = idx_curves_named)

tmpy_05 = Y05 %>% 
  as_tibble() %>% 
  slice(idx_curves) %>% 
  mutate(Quantile='50%',
         Source='FQPCA',
         link_curve = idx_curves_named)

tmpy_09 = Y09 %>% 
  as_tibble() %>% 
  slice(idx_curves) %>% 
  mutate(Quantile='90%',
         Source='FQPCA',
         link_curve = idx_curves_named)

selected_curves = tmpy %>% 
  bind_rows(tmpy_01) %>% 
  bind_rows(tmpy_05) %>% 
  bind_rows(tmpy_09)
colnames(selected_curves) = c(1:365, c('Quantile', 'Source', 'link_curve'))
 
selected_curves_long = selected_curves %>% 
  pivot_longer(all_of(1:365), names_to='Time', values_to='Accelerometer') %>% 
  mutate(identifier = paste0(link_curve, Quantile),
         Quantile = factor(Quantile, levels=c('10%', '50%', '90%', 'Original data')),
         link_curve = factor(link_curve, levels=idx_curves_named),
         Time = as.integer(Time))

colors = c(
  rgb(241, 99, 101, maxColorValue = 255),   # red
  rgb(96, 195, 200, maxColorValue = 255),   # blue
  rgb(81, 172, 85, maxColorValue = 255),    # green
  rgb(0, 0, 0, maxColorValue = 255)         # black
)

fqpca_reconstruction = selected_curves_long %>% 
  ggplot(aes(x=Time, y=Accelerometer)) + 
  geom_line(aes(group=identifier, color=Quantile, alpha=Quantile)) +
  facet_wrap(~link_curve) +
  ggtitle('FQPCA reconstruction') +
  xlab('Time') +
  ylab('Temperature') +
  theme_light() + 
  scale_color_manual(values = colors) + 
  scale_alpha_manual(values=c(1, 1, 1, 0.4)) +
  scale_size_manual(values = c(1, 1, 1, 0.5)) + 
  theme(legend.position = "bottom")

fqpca_reconstruction

```
